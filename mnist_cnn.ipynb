{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the MNIST files (Kaggle):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "with zipfile.ZipFile(\"mnist_train.csv.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall() \n",
    "\n",
    "\n",
    "print(\"Extracted files:\", os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST model (switching activations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'else' after 'if' expression (1980119682.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    device = torch.device(\"cuda\" if torch.cuda.is.available() else \"cpu\")\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'else' after 'if' expression\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#defining Tangma\n",
    "class Tangma(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tangma, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.0))\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(x + self.alpha) + self.gamma * x\n",
    "\n",
    "#defining Swish \n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class MNIST_CNN(nn.Module): \n",
    "    def  __init__(self, activation):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.25) \n",
    "\n",
    "        self.fc1 = nn.Linear(9216, 128) \n",
    "\n",
    "        self.dropout2 = nn.Dropout(0.5) \n",
    "\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) #(1, 28, 28)\n",
    "\n",
    "\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv2(x) #(64, 24, 24)\n",
    "\n",
    "\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = F.max_pool2d(x, 2) #(64, 12, 12)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = torch.flatten(x, 1) #64*12*12 = 9216\n",
    "\n",
    "        x = self.fc1(x) #(9216, 128)\n",
    "\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x) #(128, 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "#load csv dataset\n",
    "df = pd.read_csv(\"mnist_train.csv\")\n",
    "\n",
    "#all rows and columns (except first one) are normalized from [0, 255] to [0, 1] and reshapes to grayscale 28*28 \n",
    "X = torch.tensor(df.iloc[:, 1:].values / 255.0, dtype=torch.float32).view(-1, 1, 28, 28) \n",
    "y = torch.tensor(df.iloc[:, 0].values, dtype=torch.long) #converts labels to long\n",
    "\n",
    "#create TensorDataset\n",
    "full_dataset = TensorDataset(X, y)\n",
    "\n",
    "\n",
    "#split 80/20 --> train/test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size #full dataset - 80% \n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size]) #split into train and test dataset\n",
    "\n",
    "#train both --> train with batch size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) \n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "#training\n",
    "def train_model(model, name, epochs=10):\n",
    "    model.to(device)\n",
    "\n",
    "    #using Adam and 0.001 learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss() #cross-entropy loss\n",
    "\n",
    "    train_losses, val_losses, val_accuracies, times = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            #record learnable parameters for batches 130 and 260\n",
    "            if name == \"Tangma\" and batch_idx in {130, 260}:\n",
    "                print(f\"[Tangma] Epoch {epoch+1}, Batch {batch_idx} | alpha = {model.activation.alpha.item():.4f}, gamma = {model.activation.gamma.item():.4f}\")\n",
    "            \n",
    "\n",
    "            data, target = data.to(device), target.to(device) #move to GPU processing\n",
    "            optimizer.zero_grad() #clear previous gradients\n",
    "            output = model(data) #pass input data\n",
    "            loss = criterion(output, target) #loss calculator\n",
    "\n",
    "            \n",
    "            loss.backward() #backpropogation\n",
    "            optimizer.step()#update model weights\n",
    "            running_loss += loss.item() #add current loss to total running loss\n",
    "\n",
    "\n",
    "        #average loss per epoch from all batches\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "\n",
    "        #validation set evaluation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        val_accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        times.append(time.time() - start_time)\n",
    "\n",
    "\n",
    "        #display all stats\n",
    "        print(f\"[{name}] Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_accuracy:.4f}, Time={times[-1]:.2f}s\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies, times\n",
    "\n",
    "#all activations \n",
    "activations = {\n",
    "    \"Tangma\": Tangma(),\n",
    "    \"ReLU\": nn.ReLU(),\n",
    "    \"Swish\": Swish(),\n",
    "    \"GELU\": nn.GELU()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = {}\n",
    "for name, act in activations.items():\n",
    "    print(f\"\\nTraining with {name} activation:\")\n",
    "    model = MNIST_CNN(act)\n",
    "    results[name] = train_model(model, name)\n",
    "\n",
    "\n",
    "\n",
    "# plot results\n",
    "for metric_idx, metric_name in enumerate([\"Train Loss\", \"Val Loss\", \"Val Accuracy\", \"Time\"]):\n",
    "    plt.figure()\n",
    "    for name in activations:\n",
    "        plt.plot(results[name][metric_idx], label=name)\n",
    "    plt.title(metric_name + \" per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"mnist_{metric_name.lower().replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
